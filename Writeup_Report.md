##**Behavioral Cloning Project**

The goals / steps of this project are the following:

* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image_0]: ./markdown_source/pilotnet.png "PilotNet Architecture"
[image_1]: ./markdown_source/left.jpg "Left Camera"
[image_2]: ./markdown_source/center.jpg "Center Camera"
[image_3]: ./markdown_source/right.jpg "Right Camera"


## Rubric Points
### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:

    - model.py: containing python script for training and saving model. 
    - drive.py: used to control vehicle in autonomous mode.
    - model.h5: saved model with the best weights (generated by jupyter notebook). 
    - writeup_report.md: evaluation of project requirements.

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable
New model can be generated by running model.py. I used a jupyter notebook to train model.h5 because it is easier for me to manually decrease learning rate and retrain model multiple times with early stopping.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed
I implemented the original PilotNet (created by Nvidia in 2017) with extra dropout layers. The model consists of 5 convolutional layers and 4 fully connected layers (the output layer has a single node). Unlike the original paper, I implemented normalization layer as part of the preprocessing pipeline.

For convolutional layers, PilotNet employs both 5x5 kernel filters with strides of 2 and 3x3 kernel filters with no stride, both with valid padding. ReLU activation is used along with L2-Regularization in fully connected layers.

<center>

|             Layers             |                       Description                       |
|:------------------------------:|:-------------------------------------------------------:|
|              Input             | 66x200 YUV image  (normalized with mean=0, std=1)       |
|      Convolution 5x5 ReLU      | 24 filters, 2x2 stride, valid padding, outputs 31x98x24 |
|      Convolution 5x5 ReLU      | 36 filters, 2x2 stride, valid padding, outputs 14x47x36 |
|      Convolution 5x5 ReLU      | 48 filters, 2x2 stride, valid padding, outputs 5x22x48  |
|      Convolution 3x3 ReLU      | 64 filters, 1x1 stride, valid padding, outputs 3x20x64  |
|      Convolution 3x3 ReLU      | 64 filters, 1x1 stride, valid padding, outputs 1x18x64  |
|             Flatten            | 1152                                                    |
|         Fully Connected        | input: 1152, output: 100                                |
|         Fully Connected        | input: 100, output: 50                                  |
|         Fully Connected        | input: 50, output: 10                                   |
| Fully Connected (Output Layer) | input: 10, output: 1                                    |

![alt text][image_0]

PilotNet architecture as published in the original paper.
</center> 

#### 2. Attempts to reduce overfitting in the model
To reduce overfitting, I added a dropout layer (50%) after each fully-connected layer. I also applied L2-Regularization for each of these layer with l=0.001.

#### 3. Model parameter tuning
The model used an adam optimizer with learning_rate=0.0001. I chose a low learning rate so that it is easier for the model to converge, however, it made the training process a bit slower (more epochs).

#### 4. Appropriate training data

The training data I used is a combination of the sample driving data (provided by Udacity) and my own recorded data (two tracks with opposite direction). There are a total of 51,819 images (including all 'center', 'left', and 'right' angles). 
I created a dataloader using tensorflow datasets object. The dataloader take a dataframe of filenames as input and generate batches of normalized yuv images as output. The preprocessing pipeline is described as follow:

    - Remove the 'sky' area (approximately 34% vertically from top)
    - Resize image to 66x200
    - Randomly flip image
    - Convert from RGB to YUV
    - Normalize image

<center>
![alt text][image_2]

center camera

![alt text][image_1]
![alt text][image_3]

left & right cameras
</center> 